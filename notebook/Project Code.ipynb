{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor for Market Sales\n",
    "\n",
    "### Authors: Giacomo Bossi,  Emanuele Chioso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to provide a working\n",
    "forecasting model to optimize promotions and\n",
    "warehouse stocks of one of the most important\n",
    "European retailers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "We started analysing the dataset we were given,\n",
    "trying to identify correlations or patterns\n",
    "between features. Once the data analysis was\n",
    "complete we cleaned it (as explained in the next\n",
    "section).\n",
    "We then proceeded to implement some basic\n",
    "regressor algorithms in order to have a first\n",
    "glance of what the general performance on the\n",
    "dataset was using R2 score and MAE as the evaluation\n",
    "metric.\n",
    "In the end we selected a few of them and\n",
    "ensembled their predictions to obtain the final\n",
    "prediction for the test set.\n",
    "All testing was performed via holdout testing to\n",
    "get a quick result for completely new classifiers,\n",
    "and later with cross validation to get a less\n",
    "randomized evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation of all useful packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_classif\n",
    "from datetime import datetime\n",
    "from scipy.special import boxcox1p\n",
    "from scipy import stats\n",
    "from scipy.stats import norm,skew\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions defined in Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_log_to_skewed_features(df):\n",
    "    numeric_feats = []\n",
    "    for col in df.columns:\n",
    "        if(len(df[col].unique())>2 and df[col].dtype != \"object\"):\n",
    "            numeric_feats.append(col)\n",
    "    \n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew': skewed_feats})\n",
    "    skewness = skewness[abs(skewness.Skew) > 0.75]\n",
    "    skewed_features = skewness.index    \n",
    "    return skewed_features\n",
    "\n",
    "def apply_log_to_skewed_features(df,skewed_features,lambda_log = 0.15):\n",
    "    for feat in skewed_features:\n",
    "        df[feat] = boxcox1p(df[feat], lambda_log)\n",
    "    print(\"logged features:\",skewed_features)\n",
    "    return df\n",
    "\n",
    "def apply_exp_to_result(df,lambda_log = 0.15):\n",
    "    print(df[target].mean())\n",
    "    df[feat] = np.inv_boxcox1p(df[target], lambda_log)\n",
    "    print(df[target].mean())\n",
    "    return df\n",
    "\n",
    "def add_date(df):\n",
    "    date = np.array(  [ x.split('/') for x in df['Date'].values])\n",
    "    date = date.astype(np.int32)\n",
    "    df['Date'] = [ datetime(x[2],x[1],x[0])  for x in date ]\n",
    "    \n",
    "def apply_exp_to_result(df,lambda_log = 0.15):\n",
    "    return inv_boxcox1p(df, lambda_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/original_train.csv') \n",
    "testset = pd.read_csv('../data/original_test.csv')\n",
    "\n",
    "dataset=dataset[dataset.IsOpen != 0]\n",
    "testset=testset[testset.IsOpen != 0]\n",
    "\n",
    "\n",
    "components = dataset.columns[dataset.columns!='Unnamed: 0']\n",
    "tcomponents = testset.columns[testset.columns!='Unnamed: 0']\n",
    "\n",
    "features=set(components).intersection(tcomponents)\n",
    "wtarget=list(set(components)-set(tcomponents))\n",
    "target = 'NumberOfSales'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with NAN\n",
    "We have substituted the missing values in\n",
    "Max_Gust_Speed with the values of Max_Wind.\n",
    "Then, in order to fill all the missing values, we\n",
    "have grouped the dataset by the StoreID and\n",
    "after that, we have used a linear interpolation\n",
    "taking as index the time feature.Since the\n",
    "missing values of ‘Events’ are NMAR we haven’t\n",
    "handle it.\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_date(dataset)\n",
    "for val in dataset['StoreID'].unique():\n",
    "    df = pd.DataFrame(dataset.loc[dataset['StoreID'] == val])\n",
    "    df.index = df['Date']\n",
    "    \n",
    "    df['tOpen']=df['IsOpen'].shift(-1).fillna(method='ffill')\n",
    "    df['yOpen']=df['IsOpen'].shift(+1).fillna(method='bfill')\n",
    "    df['tPromotions']=df['HasPromotions'].shift(-1).fillna(method='ffill')\n",
    "    df['yPromotions']=df['HasPromotions'].shift(+1).fillna(method='bfill')\n",
    "    df = df.interpolate(method='time',downcast='infer',limit=10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset.drop(dataset.loc[dataset['StoreID'] == val].index, inplace=True)\n",
    "    df.index = df['StoreID']\n",
    "    dataset = pd.concat([dataset, df],ignore_index=True)\n",
    "\n",
    "    \n",
    "dataset['Precipitationmm'] = (np.ceil(dataset.Precipitationmm / 10) * 1).astype(int)\n",
    "dataset['CloudCover'] = dataset['CloudCover'].fillna(dataset['Precipitationmm'])\n",
    "dataset['Max_Gust_SpeedKm_h'] = dataset['Max_Gust_SpeedKm_h'].fillna(dataset['Max_Wind_SpeedKm_h'])\n",
    "\n",
    "#Convert some data to integer\n",
    "col_to_int = ['Min_VisibilitykM','Max_VisibilityKm','Max_Gust_SpeedKm_h',\n",
    "              'CloudCover','Mean_VisibilityKm','HasPromotions','IsHoliday','HasPromotions']\n",
    "for col in col_to_int:\n",
    "    dataset[col] = dataset[col].astype(int)\n",
    "\n",
    "#Convert some data to int since they are One Hot Encoded\n",
    "    \n",
    "#Add some datas about time\n",
    "dataset['Month'] = pd.DatetimeIndex(dataset['Date']).month\n",
    "dataset['Daysmonth']= pd.DatetimeIndex(dataset['Date']).day\n",
    "dataset['Daysweek']= pd.DatetimeIndex(dataset['Date']).dayofweek\n",
    "dataset['Quarter']= pd.DatetimeIndex(dataset['Date']).quarter\n",
    "dataset['Year']= pd.DatetimeIndex(dataset['Date']).year\n",
    "\n",
    "\n",
    "dataset.drop(columns='Date', inplace=True)\n",
    "dataset.drop(columns='IsOpen', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_date(testset)\n",
    "\n",
    "\n",
    "for val in testset['StoreID'].unique():\n",
    "    print(val,testset.shape)\n",
    "    df = pd.DataFrame(testset.loc[testset['StoreID'] == val])\n",
    "    df.index = df['Date']\n",
    "    \n",
    "    df['tOpen']=df['IsOpen'].shift(-1).fillna(method='ffill')\n",
    "    df['yOpen']=df['IsOpen'].shift(+1).fillna(method='bfill')\n",
    "    df['tPromotions']=df['HasPromotions'].shift(-1).fillna(method='ffill')\n",
    "    df['yPromotions']=df['HasPromotions'].shift(+1).fillna(method='bfill')\n",
    "    df = df.interpolate(method='time',downcast='infer', limit=100)\n",
    "    \n",
    "    \n",
    "    testset.drop(testset.loc[testset['StoreID'] == val].index, inplace=True)\n",
    "    df.index = df['StoreID']\n",
    "    print(val,df.shape)\n",
    "\n",
    "    testset = pd.concat([testset, df],ignore_index=True)\n",
    "    print(val,testset.shape)\n",
    "print(testset.shape)\n",
    "testset['Precipitationmm'] = (np.ceil(testset.Precipitationmm / 10) * 1).astype(int)\n",
    "testset['CloudCover'] = testset['CloudCover'].fillna(testset['Precipitationmm'])\n",
    "testset['Max_Gust_SpeedKm_h'] = testset['Max_Gust_SpeedKm_h'].fillna(testset['Max_Wind_SpeedKm_h'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['Min_VisibilitykM']=testset['Min_VisibilitykM'].fillna(testset['Min_VisibilitykM'].mean())\n",
    "testset['Max_VisibilityKm']=testset['Max_VisibilityKm'].fillna(testset['Max_VisibilityKm'].mean())\n",
    "testset['Mean_VisibilityKm']=testset['Mean_VisibilityKm'].fillna(testset['Mean_VisibilityKm'].mean())\n",
    "\n",
    "#Convert some data to integer\n",
    "col_to_int = ['Min_VisibilitykM','Max_VisibilityKm','Max_Gust_SpeedKm_h',\n",
    "              'CloudCover','Mean_VisibilityKm','HasPromotions','IsHoliday',\n",
    "              'Region','Region_AreaKM2','Region_GDP','Region_PopulationK']\n",
    "for col in col_to_int:\n",
    "    testset[col] = testset[col].astype(int)\n",
    "\n",
    "#Add some datas about time\n",
    "testset['Month'] = pd.DatetimeIndex(testset['Date']).month\n",
    "testset['Daysmonth']= pd.DatetimeIndex(testset['Date']).day\n",
    "testset['Daysweek']= pd.DatetimeIndex(testset['Date']).dayofweek\n",
    "testset['Quarter']= pd.DatetimeIndex(testset['Date']).quarter\n",
    "testset['Year']= pd.DatetimeIndex(testset['Date']).year\n",
    "\n",
    "testset.drop(columns='Date', inplace=True)\n",
    "testset.drop(columns='IsOpen', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the remained missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = (testset.isnull().sum() / len(testset)) * 100\n",
    "train_tmp = train_tmp.drop(train_tmp[train_tmp == 0].index).sort_values(ascending=False)[:100]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :train_tmp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis and Reduction\n",
    "## Weather Features\n",
    "In order to reduce the number of parameters\n",
    "bound to the weather features and augment the\n",
    "information associated with a single feature we\n",
    "have performed a Principal Component\n",
    "Analysis.\n",
    "We can see in this Heatmap the strong\n",
    "correlations between the weather features\n",
    "Considering only the first 4 components we\n",
    "have reached a cumulative variance of ~98%.\n",
    "So, we have reduced 20 different features into 4,\n",
    "loosing only a 2% of information. Before and\n",
    "after the PCA we have also performed a\n",
    "normalization of the parameters to attenuate\n",
    "the sensibility of this analysis to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheather_features = ['Max_Humidity', 'Max_Sea_Level_PressurehPa', 'Max_TemperatureC',\n",
    "       'Max_VisibilityKm', 'Max_Wind_SpeedKm_h', 'Mean_Dew_PointC',\n",
    "       'Mean_Humidity', 'Mean_Sea_Level_PressurehPa', 'Mean_TemperatureC','CloudCover',\n",
    "       'Mean_VisibilityKm', 'Mean_Wind_SpeedKm_h', 'Min_Dew_PointC', 'Max_Dew_PointC', \n",
    "       'Min_Humidity', 'Min_Sea_Level_PressurehPa', 'Min_TemperatureC',\n",
    "       'Min_VisibilitykM', 'Precipitationmm', 'WindDirDegrees','Max_Gust_SpeedKm_h']\n",
    "full_pca_model = PCA()\n",
    "\n",
    "n_dataset = dataset.shape[0]\n",
    "n_testset = testset.shape[0]\n",
    "\n",
    "superset = pd.concat([dataset,testset]).reset_index(drop=True)\n",
    "superset[wheather_features] = preprocessing.normalize(superset[wheather_features])\n",
    "\n",
    "full_fitted_model = full_pca_model.fit(superset[wheather_features])\n",
    "\n",
    "\n",
    "corr = superset[weather_features].corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.semilogy(full_fitted_model.explained_variance_ratio_, '--o')\n",
    "plt.xticks(np.arange(0,len(wheather_features),1))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.semilogy(full_fitted_model.explained_variance_ratio_.cumsum(), '--o')\n",
    "plt.xticks(np.arange(0,len(wheather_features),1))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "\n",
    "\n",
    "PCA_components=4\n",
    "feature_selection_pca_model = PCA(n_components=PCA_components, svd_solver='full')\n",
    "\n",
    "fitted_model = feature_selection_pca_model.fit(superset[wheather_features])\n",
    "X_selected_features_pca = fitted_model.transform(superset[wheather_features])\n",
    "\n",
    "toAdd = pd.DataFrame(X_selected_features_pca) \n",
    "preprocessing.normalize(toAdd,axis=0)\n",
    "for i in range(0,PCA_components):\n",
    "    superset['wheather_PCA_'+str(i)]= toAdd[i]\n",
    "superset.drop(columns=wheather_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region\n",
    "We have performed the same transformation\n",
    "even to the features of the region. We have\n",
    "reduced the four features of a region into 2\n",
    "features, loosing less than 4% of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the number of region features\n",
    "region_features = ['Region_AreaKM2','Region_GDP','Region_PopulationK']\n",
    "\n",
    "superset[region_features] = preprocessing.normalize(superset[region_features])\n",
    "\n",
    "full_fitted_model = full_pca_model.fit(superset[region_features])\n",
    "\n",
    "corr = superset[region_features].corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.semilogy(full_fitted_model.explained_variance_ratio_, '--o')\n",
    "plt.xticks(np.arange(0,len(region_features),1))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.semilogy(full_fitted_model.explained_variance_ratio_.cumsum(), '--o')\n",
    "plt.xticks(np.arange(0,len(region_features),1))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "\n",
    "PCA_components=2\n",
    "feature_selection_pca_model = PCA(n_components=PCA_components, svd_solver='full')\n",
    "\n",
    "fitted_model = feature_selection_pca_model.fit(superset[region_features])\n",
    "X_selected_features_pca = fitted_model.transform(superset[region_features])\n",
    "\n",
    "\n",
    "toAdd = pd.DataFrame(X_selected_features_pca) \n",
    "preprocessing.normalize(toAdd,axis=0)\n",
    "\n",
    "for i in range(0,PCA_components):\n",
    "    superset['region_PCA_'+str(i)]= toAdd[i]\n",
    "superset.drop(columns=region_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHE One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EXCEPTION FOR DAYS AND MONTHS\n",
    "for col in superset.columns:\n",
    "    if (superset[col].dtypes == 'object'):\n",
    "        for elem in superset[col].unique():\n",
    "            elem = str(elem)\n",
    "            superset[col+'_'+elem] = superset[col].apply(lambda x: 1 if str(x)==elem else 0).values.astype(float)\n",
    "        superset.drop(columns=col,inplace=True)\n",
    "        \n",
    "dataset = superset[:n_dataset]\n",
    "testset = superset[n_dataset:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distibution of the Target\n",
    "## Skewness removing\n",
    "After some analysis, we have noticed that some\n",
    "variables and also the target were skewed. So,\n",
    "trying to fit a gaussian distribution we have\n",
    "noticed some differences. As we notice below\n",
    "for the target variable, the distribution of the\n",
    "target was right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "sns.distplot(dataset['NumberOfSales'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(dataset['NumberOfSales'])\n",
    "\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('NumberOfSales distribution')\n",
    "\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "res = stats.probplot(dataset['NumberOfSales'], plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_feat = search_log_to_skewed_features(dataset)\n",
    "dataset = apply_log_to_skewed_features(dataset,sk_feat)\n",
    "sk_feat = set(sk_feat)-set(['NumberOfSales', 'NumberOfCustomers'])\n",
    "testset = apply_log_to_skewed_features(testset,sk_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have decided to apply the log\n",
    "transformation to all the variables that had a\n",
    "skewness greater than 0,75. The result obtained\n",
    "for the target are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "sns.distplot(dataset['NumberOfSales'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(dataset['NumberOfSales'])\n",
    "\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('NumberOfSales distribution')\n",
    "\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "res = stats.probplot(dataset['NumberOfSales'], plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = dataset.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corr, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## Random Forest Selection\n",
    "To select the best features found during the preprocessing we have done several features\n",
    "selection, as PCA feature selection, Correlation\n",
    "based features selection and Random Forest\n",
    "features selection. Since the best model found\n",
    "was a XGBoost we have used a Random Forest\n",
    "features selection. The threshold was set at 2 ∙ Median,\n",
    "in order to\n",
    "take all the features before the step in the\n",
    "middle (~0,02). So, we have selected the first 21\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "components = dataset.columns#[dataset.dtypes != 'object']\n",
    "features=list(set(components) - set(wtarget))\n",
    "#dataset[features] = dataset[features].values.astype(float)\n",
    "\n",
    "cv = KFold(n_splits=2, random_state=21)\n",
    "\n",
    "X = np.array(dataset[features])\n",
    "y = np.array(dataset[target])\n",
    "selected_feat = dataset[features].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "forest = ExtraTreesRegressor(n_estimators=250, random_state=0, n_jobs=-1)\n",
    "forest.fit(X, y)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], selected_feat[indices[f]], importances[indices[f]]))\n",
    "    \n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), selected_feat[indices],rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "feature_selection_model = SelectFromModel(forest, prefit=True,threshold='1.5*median')\n",
    "X_selected_features_forest = feature_selection_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected_features_forest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(testset[features])\n",
    "X_test_selected_features_forest = feature_selection_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy',X)\n",
    "np.save('y.npy',y)\n",
    "np.save('X_selected.npy',X_selected_features_forest)\n",
    "np.save('X_test.npy',X_test)\n",
    "np.save('X_test_selected.npy',X_test_selected_features_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation\n",
    "We have trained several different models, in\n",
    "order to have a more reliable valuation of the\n",
    "best model to use. First of all, we have trained a\n",
    "simple model, KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.special import boxcox1p, inv_boxcox1p\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = { 'alpha':5e-02 }\n",
    "lasso =  Lasso(max_iter=10000, **lasso_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Boost Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'n_jobs': -1, \n",
    "              'min_child_w': 1, \n",
    "              'colsample': 0.5, \n",
    "              'bagging_seed': 10, \n",
    "              'learning_rate': 0.7, \n",
    "              'bagging_fraction': 1, \n",
    "              'min_data_in_leaf': 8,\n",
    "              'objective': 'regression', \n",
    "              'num_leaves': 400, \n",
    "              'estimators': 100, \n",
    "              'bagging_freq': 1, \n",
    "              'reg_lambda': 0.9, \n",
    "              'reg_alpha': 0.9,\n",
    "              'max_bin': 300, \n",
    "              'min_sum_hessian_in_leaf': 11}\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(**lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params ={\n",
    "            \"n_estimators\":100,\n",
    "             \"colsample\":0.5,\n",
    "             \"gamma\":0.05,\n",
    "             \"learning\":0.1,\n",
    "             \"max_dep\":30,\n",
    "             \"min_child_w\":1,\n",
    "             \"reg_alpha\":0.9,\n",
    "             \"reg_lambda\":0.8,\n",
    "             \"n_jobs\":-1 }\n",
    "\n",
    "xgb_params2 ={\n",
    "            \"n_estimators\":50,\n",
    "             \"colsample\":0.5,\n",
    "             \"gamma\":0.05,\n",
    "             \"learning\":0.1,\n",
    "             \"max_dep\":30,\n",
    "             \"min_child_w\":1,\n",
    "             \"reg_alpha\":0.9,\n",
    "             \"reg_lambda\":0.8,\n",
    "             \"n_jobs\":-1 }\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "model_xgb2 = xgb.XGBRegressor(**xgb_params2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_params = {'min_impurity_decrease': False, 'max_features': 'auto', 'oob_score': False, 'bootstrap': True, \n",
    "                     'warm_start': False, 'n_jobs': -1, 'criterion': 'mse', 'min_weight_fraction_leaf': 1e-07, \n",
    "                     'min_samples_split': 5, 'min_samples_leaf': 1, 'max_leaf_nodes': None, 'n_estimators': 50, \n",
    "                     'max_depth': 50}\n",
    "\n",
    "model_forest = RandomForestRegressor(**forest_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_preds = lasso_model.predict(X_test) \n",
    "print(\"SCORE:\", r2_score(y_test, apply_exp_to_result(lasso_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Score\n",
    "The first model trained, in order to have a\n",
    "baseline to overreach was the KNN. We have\n",
    "trained this model with a different number of\n",
    "neighbours and the best result we have\n",
    "obtained was: R2 Score ≅ 0.68, using a 10 folds\n",
    "cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "kfolds = KFold(10,shuffle=True,random_state=1234)\n",
    "\n",
    "for i in range(2,30,1):\n",
    "    neigh = KNeighborsRegressor(n_neighbors=i)\n",
    "    scores = cross_val_score(neigh, X_selected_features_forest, y, cv=kfolds)\n",
    "    print('KNN has obtained',scores.mean(),'with number of Neighboors=',i)\n",
    "    result.append((i,scores.mean()))\n",
    "plt.figure(figsize=(12,12))\n",
    "results = pd.DataFrame(result)\n",
    "plt.plot(results[0], results[1] ,linestyle='-', marker=\".\", color='green', markersize=3, label=\"R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightBoost Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb.fit(X,y)\n",
    "lgb_preds = model_lgb2.predict(X_test) \n",
    "\n",
    "print(\"SCORE:\", r2_score(y_test, apply_exp_to_result(lgb_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest.fit(X,y)\n",
    "forest_preds = model_forest.predict(X_test) \n",
    "\n",
    "print(\"SCORE:\", r2_score(y_test, apply_exp_to_result(forest_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Sore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(X,y)\n",
    "xgb_preds = model_xgb.predict(X_test) \n",
    "\n",
    "print(\"SCORE:\", r2_score(y_test, apply_exp_to_result(xgb_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = (lgb_preds+forest_preds+xgb_preds)/3\n",
    "print(\"SCORE:\", r2_score(y_test, apply_exp_to_result(mean_results))\n",
    "print(\"SCORE:\", mean_absolute_error(y_test, apply_exp_to_result(mean_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensembling\n",
    "Finally, we have tried to use metamodeling\n",
    "since the averaging of base model improves the\n",
    "results. In this approach, we have created a\n",
    "meta model based on average base models and\n",
    "used an out-of-folds prediction of these models\n",
    "to train out meta model. Since the best base\n",
    "model were: Random Forest, LightBoost, XGBoost.\n",
    "The final model is the result of an ensemble of the single models.\n",
    "The models performed very well with trainset created with a Random Sampling but in a more realistic approach, where we predicted two entire months, they have been outperformed by the ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=10):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (model_xgb, model_lgb, model_forest),\n",
    "                                                 meta_model = model_xgb2)\n",
    "stacked_averaged_models.fit(X,y)\n",
    "averaged_models_preds = stacked_averaged_models.predict(X_test)\n",
    "averaged_models_preds = apply_exp_to_result(averaged_models_preds)\n",
    "print(\"R2 Score:\", r2_score(y_train, averaged_models_preds))\n",
    "print(\"MAE Score:\", mean_absolute_error(y_train, averaged_models_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
